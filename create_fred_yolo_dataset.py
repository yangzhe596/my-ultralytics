#!/usr/bin/env python
# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# -*- coding: utf-8 -*-
"""
FRED æ•°æ®é›†è½¬æ¢ä¸º YOLO æ ¼å¼.

åŸºäº FRED å®˜æ–¹æ•°æ®é›†ç”ŸæˆæŒ‡å—ï¼Œç”Ÿæˆä¸ Ultralytics YOLO å®Œå…¨å…¼å®¹çš„æ•°æ®é›†æ ¼å¼ã€‚
æ ‡ç­¾è®¡ç®—æ–¹æ³•ä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. ä½¿ç”¨ interpolated_coordinates.txtï¼ˆåŒ…å« drone_idï¼Œæ”¯æŒå¤šç›®æ ‡è¿½è¸ªï¼‰
2. æ”¯æŒå¸§çº§åˆ«åˆ’åˆ†ï¼ˆFrame-level Splitï¼‰- æ•°æ®åˆ†å¸ƒæ›´å‡è¡¡
3. æ”¯æŒåºåˆ—çº§åˆ«åˆ’åˆ†ï¼ˆSequence-level Splitï¼‰- æ›´å¥½çš„æ³›åŒ–è¯„ä¼°
4. ç”Ÿæˆ YOLO æ ¼å¼çš„æ ‡ç­¾æ–‡ä»¶ï¼ˆ.txtï¼‰
5. åˆ›å»º YOLO æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼ˆ.yamlï¼‰
6. å®Œæ•´çš„æ•°æ®éªŒè¯å’Œç»Ÿè®¡

ä½¿ç”¨æ–¹æ³•ï¼š
    # å¸§çº§åˆ«åˆ’åˆ†ï¼ˆæ¨èï¼Œä½¿ç”¨ç¬¦å·é“¾æ¥ï¼‰
    python create_fred_yolo_dataset.py --split-mode frame --modality both

    # åºåˆ—çº§åˆ«åˆ’åˆ†ï¼ˆä½¿ç”¨ç¬¦å·é“¾æ¥ï¼‰
    python create_fred_yolo_dataset.py --split-mode sequence --modality both

    # ä»…è½¬æ¢ RGB æ¨¡æ€
    python create_fred_yolo_dataset.py --modality rgb

    # ç”Ÿæˆç®€åŒ–æ•°æ®é›†ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•å„100å¼ ï¼‰
    python create_fred_yolo_dataset.py --simple-dataset --simple-samples 100

    # å¤åˆ¶æ–‡ä»¶è€Œéä½¿ç”¨ç¬¦å·é“¾æ¥
    python create_fred_yolo_dataset.py --copy-files

    # ç¦ç”¨ç¬¦å·é“¾æ¥ï¼ˆä¸ --copy-files ç›¸åŒï¼‰
    python create_fred_yolo_dataset.py --no-use-symlinks
"""

import argparse
import hashlib
import json
import logging
import os
import random
import shutil
from collections import defaultdict
from datetime import datetime
from pathlib import Path

from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class FREDtoYOLOConverter:
    """FRED æ•°æ®é›†è½¬æ¢ä¸º YOLO æ ¼å¼."""

    def __init__(self, fred_root, output_root, split_mode="frame"):
        """åˆå§‹åŒ–è½¬æ¢å™¨.

        Args:
            fred_root: FRED æ•°æ®é›†æ ¹ç›®å½•
            output_root: è¾“å‡ºç›®å½•
            split_mode: 'frame' æˆ– 'sequence'
        """
        self.fred_root = Path(fred_root)
        self.output_root = Path(output_root)
        self.split_mode = split_mode

        if not self.fred_root.exists():
            raise FileNotFoundError(f"FRED æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {self.fred_root}")

        # YOLO ç±»åˆ«å®šä¹‰ï¼ˆä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼‰
        self.class_names = ["drone"]
        self.num_classes = len(self.class_names)

        logger.info(f"FRED æ ¹ç›®å½•: {self.fred_root}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_root}")
        logger.info(f"åˆ’åˆ†æ¨¡å¼: {split_mode}")

    def get_all_sequences(self):
        """è·å–æ‰€æœ‰å¯ç”¨çš„åºåˆ— ID."""
        sequences = []
        for seq_dir in sorted(self.fred_root.iterdir()):
            if seq_dir.is_dir() and seq_dir.name.isdigit():
                sequences.append(int(seq_dir.name))
        return sorted(sequences)

    def load_annotations(self, sequence_path):
        """åŠ è½½æ ‡æ³¨æ–‡ä»¶ï¼ˆä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼‰.

        Args:
            sequence_path: åºåˆ—ç›®å½•è·¯å¾„

        Returns:
            dict: {timestamp: [{'bbox': (x1,y1,x2,y2), 'drone_id': id}, ...]}
        """
        # ä¼˜å…ˆä½¿ç”¨æ’å€¼æ ‡æ³¨æ–‡ä»¶
        annotation_file = sequence_path / "interpolated_coordinates.txt"

        if not annotation_file.exists():
            logger.warning("æœªæ‰¾åˆ° interpolated_coordinates.txtï¼Œå°è¯•ä½¿ç”¨ coordinates.txt")
            annotation_file = sequence_path / "coordinates.txt"

        if not annotation_file.exists():
            logger.warning(f"åºåˆ— {sequence_path.name} æ— æ ‡æ³¨æ–‡ä»¶")
            return {}

        annotations = {}

        with open(annotation_file) as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                try:
                    # è§£ææ ¼å¼: "æ—¶é—´: x1, y1, x2, y2, id" æˆ– "æ—¶é—´: x1, y1, x2, y2"
                    time_str, coords_str = line.split(": ")
                    timestamp = float(time_str)

                    coords = [x.strip() for x in coords_str.split(",")]

                    x1, y1, x2, y2 = map(float, coords[:4])
                    drone_id = int(float(coords[4])) if len(coords) > 4 else 1

                    # éªŒè¯è¾¹ç•Œæ¡†æœ‰æ•ˆæ€§
                    if x2 <= x1 or y2 <= y1:
                        logger.warning(f"{annotation_file.name} ç¬¬ {line_num} è¡Œ: æ— æ•ˆè¾¹ç•Œæ¡† ({x1},{y1},{x2},{y2})")
                        continue

                    if timestamp not in annotations:
                        annotations[timestamp] = []

                    annotations[timestamp].append({"bbox": (x1, y1, x2, y2), "drone_id": drone_id})

                except Exception as e:
                    logger.warning(f"{annotation_file.name} ç¬¬ {line_num} è¡Œè§£æå¤±è´¥: {e}")
                    continue

        logger.info(f"åºåˆ— {sequence_path.name}: åŠ è½½ {len(annotations)} ä¸ªæ—¶é—´æˆ³çš„æ ‡æ³¨")
        return annotations

    def get_frames(self, sequence_path, modality):
        """è·å–å¸§åˆ—è¡¨åŠå…¶æ—¶é—´æˆ³ï¼ˆä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼‰.

        Args:
            sequence_path: åºåˆ—ç›®å½•è·¯å¾„
            modality: 'rgb' æˆ– 'event'

        Returns:
            list: [(timestamp, frame_path), ...]
        """
        if modality == "rgb":
            # ä½¿ç”¨ PADDED_RGBï¼ˆä¸ Event å¯¹é½ï¼‰
            frame_dir = sequence_path / "PADDED_RGB"
            if not frame_dir.exists():
                # å›é€€åˆ°åŸå§‹ RGB
                frame_dir = sequence_path / "RGB"
            pattern = "*.jpg"
        elif modality == "event":
            frame_dir = sequence_path / "Event" / "Frames"
            pattern = "*.png"
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡æ€: {modality}")

        if not frame_dir.exists():
            logger.warning(f"å¸§ç›®å½•ä¸å­˜åœ¨: {frame_dir}")
            return []

        frames = []

        for frame_path in sorted(frame_dir.glob(pattern)):
            timestamp = self._extract_timestamp(frame_path.name, modality)
            if timestamp is not None:
                frames.append((timestamp, frame_path))

        # æŒ‰æ—¶é—´æˆ³æ’åº
        frames = sorted(frames, key=lambda x: x[0])

        # è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´æˆ³
        if frames:
            first_timestamp = frames[0][0]
            frames = [(t - first_timestamp, path) for t, path in frames]

        return frames

    def _extract_timestamp(self, filename, modality):
        """ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³ï¼ˆä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼‰."""
        try:
            if modality == "rgb":
                # Video_0_16_03_03.363444.jpg
                name = filename.replace(".jpg", "")
                parts = name.split("_")

                if len(parts) >= 4:
                    time_parts = parts[-3:]
                    hours = int(time_parts[0])
                    minutes = int(time_parts[1])
                    seconds = float(time_parts[2])

                    return hours * 3600 + minutes * 60 + seconds

            elif modality == "event":
                # Video_0_frame_100032333.png
                name = filename.replace(".png", "")
                parts = name.split("_")

                if len(parts) >= 3:
                    timestamp_us = int(parts[-1])
                    return timestamp_us / 1_000_000

        except Exception as e:
            logger.warning(f"æ— æ³•ä»æ–‡ä»¶å '{filename}' æå–æ—¶é—´æˆ³: {e}")

        return None

    def find_closest_annotation(self, timestamp, annotations, threshold=0.05):
        """æŸ¥æ‰¾æœ€æ¥è¿‘çš„æ ‡æ³¨ï¼ˆä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼‰.

        Args:
            timestamp: ç›®æ ‡æ—¶é—´æˆ³
            annotations: æ ‡æ³¨å­—å…¸
            threshold: æ—¶é—´å®¹å·®ï¼ˆç§’ï¼‰

        Returns:
            list: æ ‡æ³¨åˆ—è¡¨æˆ–ç©ºåˆ—è¡¨
        """
        if not annotations:
            return []

        closest_time = min(annotations.keys(), key=lambda t: abs(t - timestamp))

        if abs(closest_time - timestamp) <= threshold:
            return annotations[closest_time]

        return []

    def validate_bbox(self, bbox, width, height):
        """éªŒè¯å¹¶ä¿®æ­£è¾¹ç•Œæ¡†ï¼ˆä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼‰.

        Args:
            bbox: (x1, y1, x2, y2)
            width: å›¾åƒå®½åº¦
            height: å›¾åƒé«˜åº¦

        Returns:
            tuple: (is_valid, corrected_bbox)
        """
        x1, y1, x2, y2 = bbox

        # ç¡®ä¿åæ ‡é¡ºåºæ­£ç¡®
        if x2 < x1:
            x1, x2 = x2, x1
        if y2 < y1:
            y1, y2 = y2, y1

        # é™åˆ¶åœ¨å›¾åƒè¾¹ç•Œå†…
        x1 = max(0, min(x1, width - 1))
        y1 = max(0, min(y1, height - 1))
        x2 = max(0, min(x2, width))
        y2 = max(0, min(y2, height))

        # ç¡®ä¿æœ‰æ•ˆé¢ç§¯
        if x2 <= x1 or y2 <= y1:
            return False, None

        return True, (x1, y1, x2, y2)

    def convert_bbox_to_yolo(self, bbox, width, height):
        """å°†è¾¹ç•Œæ¡†è½¬æ¢ä¸º YOLO æ ¼å¼.

        Args:
            bbox: (x1, y1, x2, y2) è¾¹ç•Œæ¡†åæ ‡
            width: å›¾åƒå®½åº¦
            height: å›¾åƒé«˜åº¦

        Returns:
            tuple: (class_id, x_center, y_center, bbox_width, bbox_height) å½’ä¸€åŒ–åæ ‡
        """
        x1, y1, x2, y2 = bbox

        # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå®½é«˜
        x_center = (x1 + x2) / 2.0
        y_center = (y1 + y2) / 2.0
        bbox_width = x2 - x1
        bbox_height = y2 - y1

        # å½’ä¸€åŒ–
        x_center /= width
        y_center /= height
        bbox_width /= width
        bbox_height /= height

        # ç¡®ä¿åœ¨ [0, 1] èŒƒå›´å†…
        x_center = max(0, min(x_center, 1.0))
        y_center = max(0, min(y_center, 1.0))
        bbox_width = max(0, min(bbox_width, 1.0))
        bbox_height = max(0, min(bbox_height, 1.0))

        # ç±»åˆ« IDï¼ˆdrone = 0ï¼‰
        class_id = 0

        return class_id, x_center, y_center, bbox_width, bbox_height

    def process_sequence(self, sequence_id, modality, output_dir, split, use_symlinks=True):
        """å¤„ç†å•ä¸ªåºåˆ—ï¼Œç”Ÿæˆ YOLO æ ¼å¼çš„å›¾åƒå’Œæ ‡ç­¾.

        Args:
            sequence_id: åºåˆ— ID
            modality: 'rgb' æˆ– 'event'
            output_dir: è¾“å‡ºç›®å½•
            split: 'train', 'val', æˆ– 'test'
            use_symlinks: æ˜¯å¦ä½¿ç”¨ç¬¦å·é“¾æ¥/ç›¸å¯¹è·¯å¾„è€Œéå¤åˆ¶æ–‡ä»¶

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        sequence_path = self.fred_root / str(sequence_id)

        if not sequence_path.exists():
            logger.warning(f"åºåˆ— {sequence_id} ä¸å­˜åœ¨")
            return {}

        # åˆ›å»ºè¾“å‡ºç›®å½•
        images_dir = output_dir / "images" / split
        labels_dir = output_dir / "labels" / split
        images_dir.mkdir(parents=True, exist_ok=True)
        labels_dir.mkdir(parents=True, exist_ok=True)

        # åŠ è½½æ ‡æ³¨
        annotations_dict = self.load_annotations(sequence_path)

        # è·å–å¸§
        frames = self.get_frames(sequence_path, modality)

        if not frames:
            logger.warning(f"åºåˆ— {sequence_id} ({modality}) æ— å¸§")
            return {}

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_frames": len(frames),
            "matched_frames": 0,
            "total_annotations": 0,
            "invalid_bboxes": 0,
            "processed_images": 0,
        }

        # å›¾åƒå°ºå¯¸ï¼ˆFRED æ•°æ®é›†å›ºå®šå°ºå¯¸ï¼‰
        width, height = 1280, 720

        # åˆ›å»ºå›¾åƒåˆ—è¡¨æ–‡ä»¶ï¼ˆè®°å½•å›¾åƒè·¯å¾„ï¼‰
        image_list_file = images_dir / f"{split}_images.txt"

        for timestamp, frame_path in tqdm(frames, desc=f"å¤„ç†åºåˆ— {sequence_id} ({modality})"):
            # æŸ¥æ‰¾åŒ¹é…çš„æ ‡æ³¨
            anns = self.find_closest_annotation(timestamp, annotations_dict)

            # ä»…åŒ…å«æœ‰æ ‡æ³¨çš„å¸§
            if not anns:
                continue

            stats["matched_frames"] += 1

            # å¤„ç†å›¾åƒè·¯å¾„
            image_ext = frame_path.suffix
            output_image_name = f"{sequence_id:04d}_{timestamp:.6f}{image_ext}"

            # ä½¿ç”¨ç»å¯¹è·¯å¾„åˆ›å»ºç¬¦å·é“¾æ¥
            absolute_path = str(frame_path.resolve())

            if use_symlinks:
                # åˆ›å»ºç¬¦å·é“¾æ¥åˆ°ç»å¯¹è·¯å¾„
                output_image_path = images_dir / output_image_name

                try:
                    # åˆ›å»ºç»å¯¹è·¯å¾„çš„ç¬¦å·é“¾æ¥
                    os.symlink(absolute_path, output_image_path)
                    stats["processed_images"] += 1
                except (OSError, NotImplementedError) as e:
                    # å¦‚æœç¬¦å·é“¾æ¥å¤±è´¥ï¼Œåˆ›å»ºæ–‡æœ¬æ–‡ä»¶è®°å½•è·¯å¾„
                    logger.warning(f"åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥ï¼Œä½¿ç”¨.pathæ–‡ä»¶: {e}")
                    link_file = images_dir / f"{output_image_name}.path"
                    with open(link_file, "w") as f:
                        f.write(absolute_path)
                    stats["processed_images"] += 1
            else:
                # å¤åˆ¶å›¾åƒæ–‡ä»¶ï¼ˆåŸå§‹æ–¹æ³•ï¼‰
                output_image_path = images_dir / output_image_name
                try:
                    shutil.copy2(frame_path, output_image_path)
                    stats["processed_images"] += 1
                except Exception as e:
                    logger.warning(f"å¤åˆ¶å›¾åƒå¤±è´¥ {frame_path}: {e}")
                    continue

            # å°†å›¾åƒè·¯å¾„æ·»åŠ åˆ°åˆ—è¡¨æ–‡ä»¶ï¼ˆä½¿ç”¨ç›¸å¯¹äºæ•°æ®é›†æ ¹ç›®å½•çš„è·¯å¾„ï¼‰
            try:
                list_path = os.path.relpath(frame_path, output_dir.parent)
            except ValueError:
                list_path = str(frame_path)

            with open(image_list_file, "a") as f:
                f.write(f"{list_path}\n")

            # åˆ›å»ºå¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶ï¼ˆä½¿ç”¨å›¾åƒæ–‡ä»¶çš„åŸºç¡€åç§°ï¼‰
            output_label_path = labels_dir / f"{Path(output_image_name).stem}.txt"

            with open(output_label_path, "w") as f:
                for ann in anns:
                    bbox = ann["bbox"]

                    # éªŒè¯è¾¹ç•Œæ¡†
                    is_valid, corrected_bbox = self.validate_bbox(bbox, width, height)

                    if not is_valid:
                        stats["invalid_bboxes"] += 1
                        continue

                    # è½¬æ¢ä¸º YOLO æ ¼å¼
                    class_id, x_center, y_center, bbox_width, bbox_height = self.convert_bbox_to_yolo(
                        corrected_bbox, width, height
                    )

                    # å†™å…¥ YOLO æ ¼å¼çš„æ ‡ç­¾
                    f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\n")
                    stats["total_annotations"] += 1

        return stats

    def split_sequences(self, sequences, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):
        """åºåˆ—çº§åˆ«åˆ’åˆ†ï¼ˆä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼‰.

        Args:
            sequences: åºåˆ— ID åˆ—è¡¨
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            seed: éšæœºç§å­

        Returns:
            tuple: (train_seqs, val_seqs, test_seqs)
        """
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6

        random.seed(seed)
        sequences = sequences.copy()
        random.shuffle(sequences)

        n_total = len(sequences)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)

        train_seqs = sequences[:n_train]
        val_seqs = sequences[n_train : n_train + n_val]
        test_seqs = sequences[n_train + n_val :]

        return sorted(train_seqs), sorted(val_seqs), sorted(test_seqs)

    def get_frame_split(self, sequence_id, frame_idx, train_ratio=0.7, val_ratio=0.15, seed=42):
        """å¸§çº§åˆ«åˆ’åˆ†ï¼ˆä¸ convert_fred_to_coco_v2.py ä¿æŒä¸€è‡´ï¼‰.

        Args:
            sequence_id: åºåˆ— ID
            frame_idx: å¸§ç´¢å¼•
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            seed: éšæœºç§å­

        Returns:
            str: 'train', 'val', æˆ– 'test'
        """
        hash_input = f"{sequence_id}_{frame_idx}_{seed}"
        hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)
        rand_val = (hash_value % 1000000) / 1000000.0

        if rand_val < train_ratio:
            return "train"
        elif rand_val < train_ratio + val_ratio:
            return "val"
        else:
            return "test"

    def create_dataset_yaml(self, output_dir, modality):
        """åˆ›å»º YOLO æ•°æ®é›†é…ç½®æ–‡ä»¶.

        Args:
            output_dir: è¾“å‡ºç›®å½•
            modality: 'rgb' æˆ– 'event'
        """
        dataset_name = f"fred_{modality}"

        yaml_content = f"""# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# FRED {modality.upper()} dataset converted from FRED format
# Generated by create_fred_yolo_dataset.py
# Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

# Train/val/test sets
path: {dataset_name}  # dataset root dir
train: images/train  # train images (relative to 'path')
val: images/val  # val images (relative to 'path')
test: images/test  # test images (optional)

# Classes
names:
"""

        for i, name in enumerate(self.class_names):
            yaml_content += f"  {i}: {name}\n"

        yaml_file = output_dir / f"{dataset_name}.yaml"
        with open(yaml_file, "w") as f:
            f.write(yaml_content)

        logger.info(f"âœ“ æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»º: {yaml_file}")

    def generate_simple_dataset(self, modality="rgb", num_samples=100, seed=42, use_symlinks=True):
        """ç”Ÿæˆç®€åŒ–çš„ YOLO æ•°æ®é›†ï¼Œç”¨äºå¿«é€ŸéªŒè¯.

        Args:
            modality: 'rgb' æˆ– 'event'
            num_samples: æ¯ä¸ªåˆ’åˆ†çš„æ ·æœ¬æ•°é‡
            seed: éšæœºç§å­
            use_symlinks: æ˜¯å¦ä½¿ç”¨ç¬¦å·é“¾æ¥/ç›¸å¯¹è·¯å¾„è€Œéå¤åˆ¶æ–‡ä»¶
        """
        logger.info(f"\n{'=' * 70}")
        logger.info("FRED è½¬ YOLO - ç®€åŒ–æ•°æ®é›†ç”Ÿæˆ")
        logger.info(f"{'=' * 70}")
        logger.info(f"æ¨¡æ€: {modality}")
        logger.info(f"æ¯ä¸ªåˆ’åˆ†æ ·æœ¬æ•°: {num_samples}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        dataset_dir = self.output_root / f"fred_{modality}_simple"
        dataset_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå›¾åƒå’Œæ ‡ç­¾ç›®å½•
        for split in ["train", "val", "test"]:
            (dataset_dir / "images" / split).mkdir(parents=True, exist_ok=True)
            (dataset_dir / "labels" / split).mkdir(parents=True, exist_ok=True)

        # è·å–æ‰€æœ‰åºåˆ—
        sequences = self.get_all_sequences()

        # æ”¶é›†æ‰€æœ‰å¸§
        all_frames = []
        for seq_id in sequences:
            sequence_path = self.fred_root / str(seq_id)
            frames = self.get_frames(sequence_path, modality)

            for frame_idx, (timestamp, frame_path) in enumerate(frames):
                all_frames.append((seq_id, timestamp, frame_path))

        # éšæœºæ‰“ä¹±å¹¶é€‰æ‹©æ ·æœ¬
        random.seed(seed)
        random.shuffle(all_frames)

        # é€‰æ‹©æ€»æ ·æœ¬æ•°ï¼ˆæ¯ä¸ªåˆ’åˆ†num_samplesï¼Œå…±3*num_samplesï¼‰
        total_samples = num_samples * 3
        selected_frames = all_frames[: min(total_samples, len(all_frames))]

        # å¹³å‡åˆ†é…åˆ°ä¸‰ä¸ªåˆ’åˆ†
        train_frames = selected_frames[:num_samples]
        val_frames = selected_frames[num_samples : num_samples * 2]
        test_frames = (
            selected_frames[num_samples * 2 : num_samples * 3]
            if len(selected_frames) >= num_samples * 3
            else selected_frames[num_samples * 2 :]
        )

        # å¤„ç†æ¯ä¸ªåˆ’åˆ†
        splits = [("train", train_frames), ("val", val_frames), ("test", test_frames)]

        total_stats = {"total_frames": 0, "processed_frames": 0, "total_annotations": 0, "invalid_bboxes": 0}

        for split_name, frames in splits:
            logger.info(f"\nå¤„ç† {split_name} åˆ’åˆ†: {len(frames)} å¸§")

            # åˆ›å»ºå›¾åƒåˆ—è¡¨æ–‡ä»¶ï¼ˆè®°å½•å›¾åƒè·¯å¾„ï¼‰
            images_dir = dataset_dir / "images" / split_name
            labels_dir = dataset_dir / "labels" / split_name
            image_list_file = images_dir / f"{split_name}_images.txt"

            # æŒ‰åºåˆ—åˆ†ç»„å¤„ç†
            seq_frames = defaultdict(list)
            for seq_id, timestamp, frame_path in frames:
                seq_frames[seq_id].append((timestamp, frame_path))

            for seq_id, seq_frame_list in seq_frames.items():
                sequence_path = self.fred_root / str(seq_id)
                annotations_dict = self.load_annotations(sequence_path)

                for timestamp, frame_path in tqdm(seq_frame_list, desc=f"å¤„ç†åºåˆ— {seq_id} ({split_name})"):
                    # æŸ¥æ‰¾åŒ¹é…çš„æ ‡æ³¨
                    anns = self.find_closest_annotation(timestamp, annotations_dict)

                    # ä»…åŒ…å«æœ‰æ ‡æ³¨çš„å¸§
                    if not anns:
                        continue

                    total_stats["total_frames"] += 1

                    # å¤„ç†å›¾åƒè·¯å¾„
                    image_ext = frame_path.suffix
                    output_image_name = f"{seq_id:04d}_{timestamp:.6f}{image_ext}"

                    # ä½¿ç”¨ç»å¯¹è·¯å¾„åˆ›å»ºç¬¦å·é“¾æ¥
                    absolute_path = str(frame_path.resolve())

                    # åˆ›å»ºç¬¦å·é“¾æ¥åˆ°ç»å¯¹è·¯å¾„
                    output_image_path = images_dir / output_image_name
                    try:
                        # åˆ›å»ºç»å¯¹è·¯å¾„çš„ç¬¦å·é“¾æ¥
                        os.symlink(absolute_path, output_image_path)
                        total_stats["processed_frames"] += 1
                    except (OSError, NotImplementedError) as e:
                        # å¦‚æœç¬¦å·é“¾æ¥å¤±è´¥ï¼Œåˆ›å»ºæ–‡æœ¬æ–‡ä»¶è®°å½•è·¯å¾„
                        logger.warning(f"åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥ï¼Œä½¿ç”¨.pathæ–‡ä»¶: {e}")
                        link_file = images_dir / f"{output_image_name}.path"
                        with open(link_file, "w") as f:
                            f.write(absolute_path)
                        total_stats["processed_frames"] += 1

                    # å°†å›¾åƒè·¯å¾„æ·»åŠ åˆ°åˆ—è¡¨æ–‡ä»¶ï¼ˆä½¿ç”¨ç›¸å¯¹äºæ•°æ®é›†æ ¹ç›®å½•çš„è·¯å¾„ï¼‰
                    try:
                        list_path = os.path.relpath(frame_path, dataset_dir.parent)
                    except ValueError:
                        list_path = str(frame_path)

                    with open(image_list_file, "a") as f:
                        f.write(f"{list_path}\n")

                    # åˆ›å»ºæ ‡ç­¾æ–‡ä»¶ï¼ˆä½¿ç”¨å›¾åƒæ–‡ä»¶çš„åŸºç¡€åç§°ï¼‰
                    output_label_path = labels_dir / f"{Path(output_image_name).stem}.txt"

                    width, height = 1280, 720

                    with open(output_label_path, "w") as f:
                        for ann in anns:
                            bbox = ann["bbox"]

                            # éªŒè¯è¾¹ç•Œæ¡†
                            is_valid, corrected_bbox = self.validate_bbox(bbox, width, height)

                            if not is_valid:
                                total_stats["invalid_bboxes"] += 1
                                continue

                            # è½¬æ¢ä¸º YOLO æ ¼å¼
                            class_id, x_center, y_center, bbox_width, bbox_height = self.convert_bbox_to_yolo(
                                corrected_bbox, width, height
                            )

                            # å†™å…¥ YOLO æ ¼å¼çš„æ ‡ç­¾
                            f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\n")
                            total_stats["total_annotations"] += 1

            logger.info(f"\n{split_name} ç»Ÿè®¡:")
            logger.info(f"  å¤„ç†å›¾åƒ: {len(frames)}")

        # åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
        dataset_name = f"fred_{modality}_simple"
        yaml_content = f"""# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# FRED {modality.upper()} Simple Dataset for Quick Validation
# Generated by create_fred_yolo_dataset.py
# Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
# Samples per split: {num_samples}

# Train/val/test sets
path: {dataset_name}  # dataset root dir
train: images/train  # train images (relative to 'path')
val: images/val  # val images (relative to 'path')
test: images/test  # test images (relative to 'path')

# Classes
names:
"""

        for i, name in enumerate(self.class_names):
            yaml_content += f"  {i}: {name}\n"

        yaml_file = self.output_root / f"{dataset_name}.yaml"
        with open(yaml_file, "w") as f:
            f.write(yaml_content)

        logger.info(f"âœ“ æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»º: {yaml_file}")

        # ä¿å­˜åˆ’åˆ†ä¿¡æ¯
        split_info = {
            "dataset_type": "simple",
            "samples_per_split": num_samples,
            "total_samples": total_stats["processed_frames"],
            "seed": seed,
            "modality": modality,
            "date_created": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        }

        info_file = dataset_dir / "dataset_info.json"
        with open(info_file, "w") as f:
            json.dump(split_info, f, indent=2)

        logger.info(f"\n{'=' * 70}")
        logger.info("ç®€åŒ–æ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        logger.info(f"{'=' * 70}")
        logger.info(f"æ€»å¸§æ•°: {total_stats['total_frames']}")
        logger.info(f"å¤„ç†å¸§æ•°: {total_stats['processed_frames']}")
        logger.info(f"æ€»æ ‡æ³¨: {total_stats['total_annotations']}")
        logger.info(f"æ— æ•ˆæ¡†: {total_stats['invalid_bboxes']}")

    def generate_all_splits(
        self, modality="both", train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42, use_symlinks=True
    ):
        """ç”Ÿæˆæ‰€æœ‰åˆ’åˆ†å’Œæ¨¡æ€çš„ YOLO æ•°æ®é›†.

        Args:
            modality: 'rgb', 'event', æˆ– 'both'
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            seed: éšæœºç§å­
            use_symlinks: æ˜¯å¦ä½¿ç”¨ç¬¦å·é“¾æ¥/ç›¸å¯¹è·¯å¾„è€Œéå¤åˆ¶æ–‡ä»¶
        """
        logger.info(f"\n{'=' * 70}")
        logger.info(f"FRED è½¬ YOLO - {self.split_mode.upper()} çº§åˆ«åˆ’åˆ†")
        logger.info(f"{'=' * 70}")

        modalities = ["rgb", "event"] if modality == "both" else [modality]

        for mod in modalities:
            logger.info(f"\nå¤„ç† {mod.upper()} æ¨¡æ€...")

            # åˆ›å»ºè¾“å‡ºç›®å½•
            dataset_dir = self.output_root / f"fred_{mod}"
            dataset_dir.mkdir(parents=True, exist_ok=True)

            # åˆ›å»ºå›¾åƒå’Œæ ‡ç­¾ç›®å½•
            for split in ["train", "val", "test"]:
                (dataset_dir / "images" / split).mkdir(parents=True, exist_ok=True)
                (dataset_dir / "labels" / split).mkdir(parents=True, exist_ok=True)

            # è·å–æ‰€æœ‰åºåˆ—
            sequences = self.get_all_sequences()

            if self.split_mode == "sequence":
                # åºåˆ—çº§åˆ«åˆ’åˆ†
                train_seqs, val_seqs, test_seqs = self.split_sequences(
                    sequences, train_ratio, val_ratio, test_ratio, seed
                )

                logger.info("\nåºåˆ—åˆ’åˆ†:")
                logger.info(f"  è®­ç»ƒ: {len(train_seqs)} åºåˆ—")
                logger.info(f"  éªŒè¯: {len(val_seqs)} åºåˆ—")
                logger.info(f"  æµ‹è¯•: {len(test_seqs)} åºåˆ—")

                # å¤„ç†æ¯ä¸ªåˆ’åˆ†
                for split_name, seqs in [("train", train_seqs), ("val", val_seqs), ("test", test_seqs)]:
                    total_stats = defaultdict(int)

                    for seq_id in tqdm(seqs, desc=f"å¤„ç† {split_name} åºåˆ—"):
                        stats = self.process_sequence(seq_id, mod, dataset_dir, split_name, use_symlinks=True)

                        for key, value in stats.items():
                            total_stats[key] += value

                    logger.info(f"\n{split_name} ç»Ÿè®¡:")
                    logger.info(f"  å¤„ç†å›¾åƒ: {total_stats['processed_images']}")
                    logger.info(f"  åŒ¹é…å¸§: {total_stats['matched_frames']}")
                    logger.info(f"  æ€»æ ‡æ³¨: {total_stats['total_annotations']}")
                    logger.info(f"  æ— æ•ˆæ¡†: {total_stats['invalid_bboxes']}")

            else:  # frame-level
                logger.info("\nå¸§çº§åˆ«åˆ’åˆ†...")

                # æ”¶é›†æ‰€æœ‰å¸§å¹¶åˆ’åˆ†
                all_frames = []
                for seq_id in sequences:
                    sequence_path = self.fred_root / str(seq_id)
                    frames = self.get_frames(sequence_path, mod)

                    for frame_idx, (timestamp, frame_path) in enumerate(frames):
                        split = self.get_frame_split(seq_id, frame_idx, train_ratio, val_ratio, seed)
                        all_frames.append((seq_id, timestamp, frame_path, split))

                # æŒ‰åˆ’åˆ†åˆ†ç»„
                split_frames = defaultdict(list)
                for seq_id, timestamp, frame_path, split in all_frames:
                    split_frames[split].append((seq_id, timestamp, frame_path))

                # å¤„ç†æ¯ä¸ªåˆ’åˆ†
                total_stats = defaultdict(lambda: defaultdict(int))

                for split_name, frames in split_frames.items():
                    logger.info(f"\nå¤„ç† {split_name} åˆ’åˆ†: {len(frames)} å¸§")

                    # åˆ›å»ºå›¾åƒåˆ—è¡¨æ–‡ä»¶ï¼ˆè®°å½•å›¾åƒè·¯å¾„ï¼‰
                    images_dir = dataset_dir / "images" / split_name
                    labels_dir = dataset_dir / "labels" / split_name
                    image_list_file = images_dir / f"{split_name}_images.txt"

                    # æŒ‰åºåˆ—åˆ†ç»„å¤„ç†
                    seq_frames = defaultdict(list)
                    for seq_id, timestamp, frame_path in frames:
                        seq_frames[seq_id].append((timestamp, frame_path))

                    for seq_id, seq_frame_list in seq_frames.items():
                        sequence_path = self.fred_root / str(seq_id)
                        annotations_dict = self.load_annotations(sequence_path)

                        for timestamp, frame_path in tqdm(seq_frame_list, desc=f"å¤„ç†åºåˆ— {seq_id} ({split_name})"):
                            # æŸ¥æ‰¾åŒ¹é…çš„æ ‡æ³¨
                            anns = self.find_closest_annotation(timestamp, annotations_dict)

                            # ä»…åŒ…å«æœ‰æ ‡æ³¨çš„å¸§
                            if not anns:
                                continue

                            total_stats[split_name]["matched_frames"] += 1

                            # å¤„ç†å›¾åƒè·¯å¾„
                            image_ext = frame_path.suffix
                            output_image_name = f"{seq_id:04d}_{timestamp:.6f}{image_ext}"

                            # ä½¿ç”¨ç»å¯¹è·¯å¾„åˆ›å»ºç¬¦å·é“¾æ¥
                            absolute_path = str(frame_path.resolve())

                            # åˆ›å»ºç¬¦å·é“¾æ¥åˆ°ç»å¯¹è·¯å¾„
                            output_image_path = images_dir / output_image_name
                            try:
                                # åˆ›å»ºç»å¯¹è·¯å¾„çš„ç¬¦å·é“¾æ¥
                                os.symlink(absolute_path, output_image_path)
                                total_stats[split_name]["processed_images"] += 1
                            except (OSError, NotImplementedError) as e:
                                # å¦‚æœç¬¦å·é“¾æ¥å¤±è´¥ï¼Œåˆ›å»ºæ–‡æœ¬æ–‡ä»¶è®°å½•è·¯å¾„
                                logger.warning(f"åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥ï¼Œä½¿ç”¨.pathæ–‡ä»¶: {e}")
                                link_file = images_dir / f"{output_image_name}.path"
                                with open(link_file, "w") as f:
                                    f.write(absolute_path)
                                total_stats[split_name]["processed_images"] += 1

                            # å°†å›¾åƒè·¯å¾„æ·»åŠ åˆ°åˆ—è¡¨æ–‡ä»¶ï¼ˆä½¿ç”¨ç›¸å¯¹äºæ•°æ®é›†æ ¹ç›®å½•çš„è·¯å¾„ï¼‰
                            try:
                                list_path = os.path.relpath(frame_path, dataset_dir.parent)
                            except ValueError:
                                list_path = str(frame_path)

                            with open(image_list_file, "a") as f:
                                f.write(f"{list_path}\n")

                            # åˆ›å»ºæ ‡ç­¾æ–‡ä»¶ï¼ˆä½¿ç”¨å›¾åƒæ–‡ä»¶çš„åŸºç¡€åç§°ï¼‰
                            output_label_path = labels_dir / f"{Path(output_image_name).stem}.txt"

                            width, height = 1280, 720

                            with open(output_label_path, "w") as f:
                                for ann in anns:
                                    bbox = ann["bbox"]

                                    # éªŒè¯è¾¹ç•Œæ¡†
                                    is_valid, corrected_bbox = self.validate_bbox(bbox, width, height)

                                    if not is_valid:
                                        total_stats[split_name]["invalid_bboxes"] += 1
                                        continue

                                    # è½¬æ¢ä¸º YOLO æ ¼å¼
                                    class_id, x_center, y_center, bbox_width, bbox_height = self.convert_bbox_to_yolo(
                                        corrected_bbox, width, height
                                    )

                                    # å†™å…¥ YOLO æ ¼å¼çš„æ ‡ç­¾
                                    f.write(
                                        f"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\n"
                                    )
                                    total_stats[split_name]["total_annotations"] += 1

                    logger.info(f"\n{split_name} ç»Ÿè®¡:")
                    logger.info(f"  å¤„ç†å›¾åƒ: {total_stats[split_name]['processed_images']}")
                    logger.info(f"  åŒ¹é…å¸§: {total_stats[split_name]['matched_frames']}")
                    logger.info(f"  æ€»æ ‡æ³¨: {total_stats[split_name]['total_annotations']}")
                    logger.info(f"  æ— æ•ˆæ¡†: {total_stats[split_name]['invalid_bboxes']}")

            # åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
            self.create_dataset_yaml(self.output_root, mod)

        # ä¿å­˜åˆ’åˆ†ä¿¡æ¯
        split_info = {
            "split_mode": self.split_mode,
            "train_ratio": train_ratio,
            "val_ratio": val_ratio,
            "test_ratio": test_ratio,
            "seed": seed,
            "modalities": modalities,
            "date_created": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        }

        info_file = self.output_root / "split_info.json"
        with open(info_file, "w") as f:
            json.dump(split_info, f, indent=2)

        logger.info(f"\nâœ“ åˆ’åˆ†ä¿¡æ¯å·²ä¿å­˜: {info_file}")

        # éªŒè¯ç¬¦å·é“¾æ¥ï¼ˆå¦‚æœä½¿ç”¨äº†ç¬¦å·é“¾æ¥ï¼‰
        if use_symlinks:
            logger.info(f"\n{'=' * 70}")
            logger.info("éªŒè¯ç¬¦å·é“¾æ¥...")
            logger.info(f"{'=' * 70}")

            for mod in modalities:
                validate_symlinks(self.output_root, mod)

        logger.info(f"\n{'=' * 70}")
        logger.info("è½¬æ¢å®Œæˆï¼")
        logger.info(f"{'=' * 70}")


def validate_symlinks(dataset_dir, modality):
    """éªŒè¯ç¬¦å·é“¾æ¥æ˜¯å¦æœ‰æ•ˆ.

    Args:
        dataset_dir: æ•°æ®é›†ç›®å½•
        modality: 'rgb' æˆ– 'event'

    Returns:
        dict: éªŒè¯ç»“æœç»Ÿè®¡
    """
    dataset_name = f"fred_{modality}"
    data_path = dataset_dir / dataset_name

    if not data_path.exists():
        logger.warning(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {data_path}")
        return {}

    stats = {"total_links": 0, "valid_links": 0, "broken_links": 0, "path_files": 0}

    for split in ["train", "val", "test"]:
        images_dir = data_path / "images" / split
        if not images_dir.exists():
            continue

        logger.info(f"\néªŒè¯ {modality} {split} é›†ç¬¦å·é“¾æ¥...")

        for item in images_dir.iterdir():
            if item.is_symlink():
                stats["total_links"] += 1

                # æ£€æŸ¥ç¬¦å·é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
                try:
                    if item.resolve().exists():
                        stats["valid_links"] += 1
                    else:
                        stats["broken_links"] += 1
                        logger.warning(f"å¤±æ•ˆé“¾æ¥: {item} -> {item.readlink()}")
                except Exception as e:
                    stats["broken_links"] += 1
                    logger.warning(f"é“¾æ¥é”™è¯¯ {item}: {e}")

            elif item.suffix == ".path":
                # å¤„ç†.pathæ–‡ä»¶
                stats["path_files"] += 1
                try:
                    with open(item) as f:
                        path = f.read().strip()
                        if Path(path).exists():
                            stats["valid_links"] += 1
                        else:
                            stats["broken_links"] += 1
                            logger.warning(f".pathæ–‡ä»¶ä¸­è·¯å¾„ä¸å­˜åœ¨: {path}")
                except Exception as e:
                    stats["broken_links"] += 1
                    logger.warning(f"è¯»å–.pathæ–‡ä»¶å¤±è´¥ {item}: {e}")

    logger.info(f"\n{modality} æ¨¡æ€ç¬¦å·é“¾æ¥éªŒè¯ç»“æœ:")
    logger.info(f"  æ€»é“¾æ¥æ•°: {stats['total_links']}")
    logger.info(f"  æœ‰æ•ˆé“¾æ¥: {stats['valid_links']}")
    logger.info(f"  å¤±æ•ˆé“¾æ¥: {stats['broken_links']}")
    logger.info(f"  è·¯å¾„æ–‡ä»¶: {stats['path_files']}")

    return stats


def main():
    parser = argparse.ArgumentParser(
        description="FRED æ•°æ®é›†è½¬æ¢ä¸º YOLO æ ¼å¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¸§çº§åˆ«åˆ’åˆ†ï¼ˆæ¨èï¼Œä½¿ç”¨ç¬¦å·é“¾æ¥ï¼‰
  python create_fred_yolo_dataset.py --split-mode frame --modality both
  
  # åºåˆ—çº§åˆ«åˆ’åˆ†ï¼ˆä½¿ç”¨ç¬¦å·é“¾æ¥ï¼‰
  python create_fred_yolo_dataset.py --split-mode sequence --modality both
  
  # ä»…è½¬æ¢ RGB æ¨¡æ€
  python create_fred_yolo_dataset.py --modality rgb
  
  # ç”Ÿæˆç®€åŒ–æ•°æ®é›†ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•å„100å¼ ï¼‰
  python create_fred_yolo_dataset.py --simple-dataset --simple-samples 100
  
  # å¤åˆ¶æ–‡ä»¶è€Œéä½¿ç”¨ç¬¦å·é“¾æ¥
  python create_fred_yolo_dataset.py --copy-files
  
  # ä»…éªŒè¯ç°æœ‰ç¬¦å·é“¾æ¥
  python create_fred_yolo_dataset.py --validate-only --modality both
  
  # è‡ªå®šä¹‰åˆ’åˆ†æ¯”ä¾‹
  python create_fred_yolo_dataset.py --train-ratio 0.8 --val-ratio 0.1 --test-ratio 0.1
        """,
    )

    parser.add_argument("--fred-root", type=str, default="/mnt/data/datasets/fred", help="FRED æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--output-root", type=str, default="datasets/fred_yolo", help="è¾“å‡ºç›®å½•")
    parser.add_argument(
        "--split-mode",
        type=str,
        default="frame",
        choices=["frame", "sequence"],
        help="åˆ’åˆ†æ¨¡å¼: frameï¼ˆå¸§çº§åˆ«ï¼‰æˆ– sequenceï¼ˆåºåˆ—çº§åˆ«ï¼‰",
    )
    parser.add_argument("--modality", type=str, default="both", choices=["rgb", "event", "both"], help="è½¬æ¢çš„æ¨¡æ€")
    parser.add_argument("--train-ratio", type=float, default=0.7, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--val-ratio", type=float, default=0.15, help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--test-ratio", type=float, default=0.15, help="æµ‹è¯•é›†æ¯”ä¾‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument(
        "--use-symlinks", action="store_true", default=True, help="ä½¿ç”¨ç¬¦å·é“¾æ¥/ç›¸å¯¹è·¯å¾„è€Œéå¤åˆ¶æ–‡ä»¶ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--copy-files", action="store_true", help="å¤åˆ¶æ–‡ä»¶è€Œéä½¿ç”¨ç¬¦å·é“¾æ¥ï¼ˆè¦†ç›– --use-symlinks é€‰é¡¹ï¼‰"
    )
    parser.add_argument("--validate-only", action="store_true", help="ä»…éªŒè¯ç°æœ‰ç¬¦å·é“¾æ¥ï¼Œä¸æ‰§è¡Œè½¬æ¢")
    parser.add_argument("--simple-dataset", action="store_true", help="ç”Ÿæˆç®€åŒ–æ•°æ®é›†ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•å„æŒ‡å®šæ•°é‡ï¼‰")
    parser.add_argument("--simple-samples", type=int, default=100, help="ç®€åŒ–æ•°æ®é›†æ¯ä¸ªåˆ’åˆ†çš„æ ·æœ¬æ•°é‡")

    args = parser.parse_args()

    # å¦‚æœåªæ˜¯éªŒè¯ï¼Œæ‰§è¡ŒéªŒè¯å¹¶é€€å‡º
    if args.validate_only:
        modalities = ["rgb", "event"] if args.modality == "both" else [args.modality]
        for mod in modalities:
            validate_symlinks(Path(args.output_root), mod)
        return 0

    # ç¡®å®šæ˜¯å¦ä½¿ç”¨ç¬¦å·é“¾æ¥
    use_symlinks = args.use_symlinks and not args.copy_files

    # å¦‚æœæ˜¯ç”Ÿæˆç®€åŒ–æ•°æ®é›†
    if args.simple_dataset:
        modalities = [args.modality] if args.modality != "both" else ["rgb"]  # ç®€åŒ–æ•°æ®é›†åªç”ŸæˆRGBæ¨¡æ€

        for mod in modalities:
            logger.info(f"\nç”Ÿæˆ {mod.upper()} ç®€åŒ–æ•°æ®é›†...")
            converter = FREDtoYOLOConverter(
                fred_root=args.fred_root,
                output_root=args.output_root,
                split_mode="frame",  # ç®€åŒ–æ•°æ®é›†ä½¿ç”¨å¸§çº§åˆ«åˆ’åˆ†
            )

            converter.generate_simple_dataset(
                modality=mod, num_samples=args.simple_samples, seed=args.seed, use_symlinks=use_symlinks
            )

        logger.info("\nâœ… ç®€åŒ–æ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        return 0

    # éªŒè¯æ¯”ä¾‹
    total_ratio = args.train_ratio + args.val_ratio + args.test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        logger.error(f"æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º 1.0ï¼Œå½“å‰ä¸º {total_ratio}")
        return 1

    try:
        converter = FREDtoYOLOConverter(
            fred_root=args.fred_root, output_root=args.output_root, split_mode=args.split_mode
        )

        converter.generate_all_splits(
            modality=args.modality,
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            test_ratio=args.test_ratio,
            seed=args.seed,
            use_symlinks=use_symlinks,
        )

        logger.info("\nâœ… æ‰€æœ‰è½¬æ¢å®Œæˆï¼")
        return 0

    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
